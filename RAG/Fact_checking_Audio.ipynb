{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "eFfRQqWERtqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z7n2CQ0Iyg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18521c59-fdbe-41c5-d5c1-16d31e3bc1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-zh_5mjt1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-zh_5mjt1\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=a0ccaf35326a33a8b520d52008f5399f576360fa7a93e0dd64805fed96debb08\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6smwrgk4/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.6.0\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.12.0\n",
            "env: OPENAI_API_KEY=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\n"
          ]
        }
      ],
      "source": [
        "# whisper\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pytube\n",
        "\n",
        "import pytube as pt\n",
        "import whisper\n",
        "\n",
        "# openAI\n",
        "import os\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "%env OPENAI_API_KEY=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\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the audio"
      ],
      "metadata": {
        "id": "OGjbrawKQJik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWp9vK4QJGNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0853f93-7d28-4685-e2f6-a256a87e1fb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/audio_english.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "yt = pt.YouTube(\"https://www.youtube.com/watch?v=dd1kN_myNDs&ab_channel=TwoMinutePapers\")\n",
        "stream = yt.streams.filter(only_audio=True)[0]\n",
        "stream.download(filename=\"audio_english.mp3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the large whisper model"
      ],
      "metadata": {
        "id": "NJKe8nyuPzKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jEWbs0cJPbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e05a16-722e-413d-fee0-44dba19105ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.88G/2.88G [00:25<00:00, 122MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribing audio"
      ],
      "metadata": {
        "id": "gU4qa7T6Y50c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"audio_english.mp3\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "ERSYgBgTS9t8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982354f5-4211-4ffc-fd5b-493e2be3aeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are many AI techniques that are able to look at a still image and identify objects, textures, human poses, and object parts in them really well. However, in the age of the internet, we have videos everywhere. So an important question would be how we could do the same for these animations. One of the key ideas in this paper is that the frames of these videos are not completely independent, and they share a lot of information, so after we make our initial predictions on what is where exactly, these predictions from the previous frame can almost always be reused with a little modification. Not only that, but here you can see with these results that it can also deal with momentary occlusions and is ready to track objects that rotate over time. A key part of this method is that one, it looks back and forth in these videos to update these labels. And second, it learns in a self-supervised manner, which means that all it is given is just a little more than data, and was never given a nice dataset with explicit labels of these regions and object parts that it could learn from. You can see in this comparison table that this is not the only method that works for videos, the paper contains ample comparisons against other methods, and comes out ahead of all other unsupervised methods, and on this task, it can even get quite close to supervised methods. The supervised methods are the ones that have access to these cushy labeled datasets, and therefore should come out way ahead. But they don't, which sounds like witchcraft, considering that this technique is learning on its own. However, all this greatness comes with limitations. One of the bigger ones is that even though it does extremely well, it also plateaus, meaning that we don't see a great deal of improvement if we add more training data. Now whether this is because it is doing nearly as well as it is humanly or computerly possible, or because a more general problem formulation is still possible remains a question. I hope we find out soon. Thanks for watching and for your generous support, and I'll see you next time!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarizing text"
      ],
      "metadata": {
        "id": "1ME-PBejQXIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[{\"role\": \"system\", \"content\": \"You are a succinct and accurate summarizer\",}]\n",
        "message = (str(result[\"text\"]))\n",
        "if (len(message) > 0):\n",
        "  messages.append({\"role\": \"user\", \"content\": message}, )\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages = messages,\n",
        "      temperature=0.2,\n",
        "      max_tokens=256,\n",
        "      frequency_penalty=0.0\n",
        "  )\n",
        "  reply = response.choices[0].message.content\n",
        "  print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdXGskEIbYr5",
        "outputId": "9e370c5f-6876-499b-f5f7-23affc96b1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI technique discussed in this paper can identify objects, textures, human poses, and object parts in videos, using the shared information between frames. It can track rotating objects and handle momentary occlusions. The method learns in a self-supervised manner, without a labeled dataset. It outperforms other unsupervised methods and is comparable to supervised methods. However, it plateaus with additional training data, and it's unclear whether this is due to reaching its maximum potential or if a more general problem formulation is possible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uname = \"CHE0T\"\n",
        "!git config --global user.email 'andrewwkan@gmail.com'\n",
        "!git config --global user.name '$uname'\n",
        "\n",
        "from getpass import getpass\n",
        "password = getpass('Password:')\n",
        "!git clone https://github.com/CHE0T/ViMGuard.git\n",
        "\n",
        "%cd ViMGuard\n",
        "# create a file, then add it to stage\n",
        "!git add hello.txt\n",
        "!git commit -m 'commit message'  # commit in Colab\n",
        "!git push origin master          # push to github"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM-ySMn91OEC",
        "outputId": "db58a8ec-4c0a-4a75-b415-903fc93756b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password:··········\n",
            "Cloning into 'ViMGuard'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/ViMGuard\n",
            "fatal: pathspec 'hello.txt' did not match any files\n",
            "On branch main\n",
            "\n",
            "Initial commit\n",
            "\n",
            "nothing to commit (create/copy files and use \"git add\" to track)\n",
            "error: src refspec master does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/CHE0T/ViMGuard.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf ViMGuard"
      ],
      "metadata": {
        "id": "WFtOcXVa2xeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}